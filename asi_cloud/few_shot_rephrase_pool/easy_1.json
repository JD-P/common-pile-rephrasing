{
  "messages": [
    {
      "role": "user",
      "content": "Rephrase the following passage so that it could be understood by someone at B1 intermediate understanding of English. Avoid words longer than 5-7 letters while preserving as much of the meaning as you can. Put the rephrased passage in <rephrase> tags <rephrase>like this</rephrase> so that it can be automatically extracted by an inference script.\n\n<passage>\nThe most straightforward way to implement CDT with contemporary AI would be something like:\n\n    * a generative sequence model (e.g. GPT)\n    * with a Monte Carlo Tree Search that is used to enumerate over actions and their outcomes\n    * a reward model or utility function that tells us what the relative values of different outcomes are\n\nIf this agent was trained with reinforcement learning and played Newcomb's without already having been exposed to it, we can imagine enumerating our action space (one box, two box) and our world model/policy (since these are combined in GPT) telling us that:\n\n    * If Omega put money into both boxes then we get more money by taking both boxes.\n    * If Omega put money into one box then we break even by taking the empty box.\n\nTherefore, we naively imagine a CDT agent will conclude, I should take both boxes because the counterfactuals downstream of my current branch only either break even or profit if I two box so it's pure profit. BUT, even if this scenario is one shot if the CDT agent has played an iterated game before then it can recognize itself as being in the middle of a sequence which is ambiguous between the branch where it one boxes and gets a million dollars and the branch where it two boxes and gets one thousand dollars. It would then rationally choose to collapse to the one box branch since that's the one which is higher reward. To specify a \"CDT agent\" which doesn't do this requires you to go beyond just moving the choice between boxes after the choice of agent strategy, it requires you to start adding unnatural limitations on what qualifies as a CDT agent beyond \"thing which makes decisions by enumerating the space of outcomes downstream of the current branch and then picking the action with the most high utility states downstream of it\". Basically you start having to specify that the utility function is some e.g. GOFAI-ish structure which wouldn't be able to evaluate outcomes like this in the first place because it would have no idea what's going on. CDT used as a loss function to infer an instrumental utility function that could understand the nuances of thought experiments like this in order to assign graph nodes expected utilities would end up learning TDT-like valuations from iterated settings.\n</passage>\n\nTry to preserve as much of the original meaning and information as possible during your rephrase while making it B1 intermediate reading level. Include all the information available in the original passage and go sentence by sentence."
    },
    {
      "role": "assistant",
      "content": "<rephrase>\nThe easiest way to build a CDT AI with today's tech would be to use:\n\n* A smart AI that makes text (like GPT).\n* A search method that looks at all the moves it can make and what might happen next.\n* A way to score results, to know which outcomes are good or bad.\n\nIf we teach this AI with rewards and then give it the Newcomb's problem for the first time, we can think of it looking at its two choices (take one box, or take two boxes). Its inner model of the world would say:\n\n* If Omega put money in both boxes, I get more money by taking both.\n* If Omega put money in just one box, I get nothing if I take only the empty box.\n\nSo, at first, you might think a CDT AI will decide to take both boxes. It thinks that no matter what, taking both boxes will either give it more money or the same amount. BUT, even if this is a single game, if the AI has played many games like this before, it might see that it is in a pattern. It can see a pattern where choosing one box gets one million dollars, and a pattern where choosing two boxes gets one thousand dollars. It would then wisely choose to pick only one box, because that gives a much higher reward.\n\nTo make a \"CDT AI\" that does not do this, you cannot just change when it makes its choice. You have to add strange limits on what a CDT AI can be. You have to say its scoring system is a simple, old-style AI that cannot understand complex ideas. It would not know what is going on in this game. If you use CDT as a guide to teach an AI how to value things, and the AI is smart enough to understand problems like this, it would end up learning values that are more like TDT, especially after playing many rounds of a game.\n</rephrase>"
    }
  ]
}
