import re
import json
import random
import asyncio
import aiohttp
from typing import List
from tqdm import tqdm
from transformers import AutoTokenizer
from datasets import load_dataset

rephrase_patterns = [re.compile("<rephrase>([\s\S]+)</rephrase>"),
                     re.compile("<rephrase>([\s\S]+)<rephrase>"),
                     re.compile("<rephrase>([\s\S]+)</passage>"),
                     re.compile("<rephrase>([\s\S]+)<passage>"),
                     re.compile("<rephrase>([\s\S]+)</r")]

tokenizer = AutoTokenizer.from_pretrained("common-pile/comma-v0.1-1t")
dataset = load_dataset("jdpressman/comma_v0.1_training_dataset_sample_1B")

def estimate_tokens_per_character(dataset, tokenizer):
    ratios = []
    for i in range(1000):
        choice = random.randrange(len(dataset["train"]))
        char_len = len(dataset["train"][i]["text"])
        token_len = len(tokenizer(dataset["train"][i]["text"])["input_ids"])
        try:
            ratios.append(token_len / char_len)
        except:
            ratios.append(0)
    return sum(ratios) / len(ratios)
        
# This function generated by DeepSeek R1
def split_into_passages(
    text: str,
    max_tokens: int = 350,
    tokens_per_char: float = 0.25,
    min_tokens: int = 50
) -> List[str]:
    """
    
    Split text into passages following the preprocessing rules in Pieler et al.
    https://arxiv.org/abs/2410.20796
    
    Args:
        text: Input text to split into passages
        max_tokens: Maximum token length for passages (default: 350)
        tokens_per_char: Estimated tokens per character (default: 0.25)
        min_tokens: Minimum token length for passages (default: 50)
    
    Returns:
        List of text passages
    """
    # Helper function to estimate tokens
    def estimate_tokens(chunk: str) -> int:
        return int(len(chunk) * tokens_per_char)
    
    # Step 1: Split on line breaks
    chunks = text.split('\n')
    
    # Step 2: Remove empty passages
    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]
    
    # Step 3: Split chunks exceeding max tokens on sentence boundaries
    new_chunks = []
    sentence_endings = r'(?<=[.!?])\s+'  # Regex to split after sentence-ending punctuation
    
    for chunk in chunks:
        if estimate_tokens(chunk) > max_tokens:
            # Split on sentence boundaries
            sentences = re.split(sentence_endings, chunk)
            new_chunks.extend(sentences)
        else:
            new_chunks.append(chunk)
    
    chunks = new_chunks
    
    # Step 4: Merge consecutive passages
    passages = []
    current_passage = []
    current_length = 0
    
    for chunk in chunks:
        chunk_tokens = estimate_tokens(chunk)
        
        # If chunk is too long by itself, add as standalone passage
        if chunk_tokens >= max_tokens:
            if current_passage:
                passages.append(' '.join(current_passage))
                current_passage = []
                current_length = 0
            passages.append(chunk)
            continue
            
        # Check if adding this chunk would exceed max length
        if current_length + chunk_tokens > max_tokens:
            if current_passage:
                passages.append(' '.join(current_passage))
                current_passage = []
                current_length = 0
                
        current_passage.append(chunk)
        current_length += chunk_tokens
    
    # Add the last passage if it meets minimum length
    if current_passage:
        final_passage = ' '.join(current_passage)
        if estimate_tokens(final_passage) >= min_tokens:
            passages.append(final_passage)
    
    return passages

async def rephrase_chunk(text, template_name):
    payload = {"model": "mistralai/Mistral-Small-3.2-24B-Instruct-2506",
               "temperature":1,
               "top_k":100,
               "top_p":1,
               "repetition_penalty":1,
               "max_tokens":1024,
               "prompt":text}
    async with aiohttp.ClientSession() as session:
        async with session.post('http://localhost:5001/v1/completions',
                                json=payload) as response:
            try:
                result = await response.json()
                result_text = result["choices"][0]["text"]
                if not result_text.startswith("<rephrase>"):
                    result_text = "<rephrase>" + result_text
                for pattern in rephrase_patterns:
                    try:
                        rephrase = pattern.search(result_text).group(1)
                    except:
                        continue
                    break
                if "rephrase" not in locals():
                    # Deliberately trigger error
                    for pattern in rephrase_patterns:
                        rephrase = pattern.search(result_text).group(1)
            except Exception as e:
                print(template_name, e)
                print(await response.json())
                rephrase = ""
            return {"text": rephrase, "template": template_name}

async def rephrase_sample(passages, template, template_name):
    infer_tasks = []
    for passage in passages:
        prompt = template.format(passage=passage)
        infer_tasks.append(rephrase_chunk(prompt, template_name))
    return await asyncio.gather(*infer_tasks)
        
async def main():
    index = 0
    tokens_per_char = estimate_tokens_per_character(dataset, tokenizer)
    print(f"Average Tokens Per Character: {tokens_per_char}")
    progress = tqdm(total=len(dataset["train"]))
    progress.set_description("Rephrasing Samples")
    with open("templates/easy_template.txt") as infile:
        easy_template = infile.read()
    with open("templates/hard_template.txt") as infile:
        hard_template = infile.read()
    with open("templates/wiki_template.txt") as infile:
        wiki_template = infile.read()
    with open("templates/qa_template.txt") as infile:
        qa_template = infile.read()
    while (index + 32) < len(dataset["train"]):
        infer_tasks = []
        for i in range(32):
            chunked = split_into_passages(dataset["train"][index]["text"])
            infer_tasks.append(rephrase_sample(chunked, easy_template, "easy"))
            infer_tasks.append(rephrase_sample(chunked, hard_template, "hard"))
            infer_tasks.append(rephrase_sample(chunked, wiki_template, "wiki"))
            infer_tasks.append(rephrase_sample(chunked, qa_template, "qa"))
            index += 1
        results = await asyncio.gather(*infer_tasks)
        progress.update(32)
        # TODO: Write results to file
        print(results)
    
asyncio.run(main())
