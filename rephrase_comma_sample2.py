import re
import json
import random
import asyncio
import aiohttp
from typing import List, Dict, Any
from tqdm import tqdm
from transformers import AutoTokenizer
from datasets import load_dataset
from collections import defaultdict

# Your existing patterns and tokenizer setup
rephrase_patterns = [re.compile("<rephrase>([\s\S]+)</rephrase>"),
                     re.compile("<rephrase>([\s\S]+)<rephrase>"),
                     re.compile("<rephrase>([\s\S]+)</passage>"),
                     re.compile("<rephrase>([\s\S]+)<passage>"),
                     re.compile("<rephrase>([\s\S]+)</r")]

tokenizer = AutoTokenizer.from_pretrained("common-pile/comma-v0.1-1t")
dataset = load_dataset("jdpressman/comma_v0.1_training_dataset_sample_1B")

def estimate_tokens_per_character(dataset, tokenizer):
    ratios = []
    for i in range(1000):
        choice = random.randrange(len(dataset["train"]))
        char_len = len(dataset["train"][i]["text"])
        token_len = len(tokenizer(dataset["train"][i]["text"])["input_ids"])
        try:
            ratios.append(token_len / char_len)
        except:
            ratios.append(0)
    return sum(ratios) / len(ratios)
        
# This function generated by DeepSeek R1
def split_into_passages(
    text: str,
    max_tokens: int = 350,
    tokens_per_char: float = 0.25,
    min_tokens: int = 50
) -> List[str]:
    """
    
    Split text into passages following the preprocessing rules in Pieler et al.
    https://arxiv.org/abs/2410.20796
    
    Args:
        text: Input text to split into passages
        max_tokens: Maximum token length for passages (default: 350)
        tokens_per_char: Estimated tokens per character (default: 0.25)
        min_tokens: Minimum token length for passages (default: 50)
    
    Returns:
        List of text passages
    """
    # Helper function to estimate tokens
    def estimate_tokens(chunk: str) -> int:
        return int(len(chunk) * tokens_per_char)
    
    # Step 1: Split on line breaks
    chunks = text.split('\n')
    
    # Step 2: Remove empty passages
    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]
    
    # Step 3: Split chunks exceeding max tokens on sentence boundaries
    new_chunks = []
    sentence_endings = r'(?<=[.!?])\s+'  # Regex to split after sentence-ending punctuation
    
    for chunk in chunks:
        if estimate_tokens(chunk) > max_tokens:
            # Split on sentence boundaries
            sentences = re.split(sentence_endings, chunk)
            new_chunks.extend(sentences)
        else:
            new_chunks.append(chunk)
    
    chunks = new_chunks
    
    # Step 4: Merge consecutive passages
    passages = []
    current_passage = []
    current_length = 0
    
    for chunk in chunks:
        chunk_tokens = estimate_tokens(chunk)
        
        # If chunk is too long by itself, add as standalone passage
        if chunk_tokens >= max_tokens:
            if current_passage:
                passages.append(' '.join(current_passage))
                current_passage = []
                current_length = 0
            passages.append(chunk)
            continue
            
        # Check if adding this chunk would exceed max length
        if current_length + chunk_tokens > max_tokens:
            if current_passage:
                passages.append(' '.join(current_passage))
                current_passage = []
                current_length = 0
                
        current_passage.append(chunk)
        current_length += chunk_tokens
    
    # Add the last passage if it meets minimum length
    if current_passage:
        final_passage = ' '.join(current_passage)
        if estimate_tokens(final_passage) >= min_tokens:
            passages.append(final_passage)
    
    return passages

class ResultWriter:
    def __init__(self, output_file: str):
        self.output_file = output_file
        self.buffer = []
        self.buffer_size = 50  # Write to file every 100 results
        
    async def add_result(self, result: Dict[str, Any]):
        self.buffer.append(result)
        if len(self.buffer) >= self.buffer_size:
            await self.flush()
            
    async def flush(self):
        if not self.buffer:
            return
            
        # Use aiofiles for async file writing if needed, or thread pool
        with open(self.output_file, 'a', encoding='utf-8') as f:
            for result in self.buffer:
                f.write(json.dumps(result) + '\n')
        self.buffer.clear()

async def rephrase_chunk(text: str, template_name: str, semaphore: asyncio.Semaphore) -> Dict[str, Any]:
    async with semaphore:  # Limit concurrent requests
        payload = {
            "model": "mistralai/Mistral-Small-3.2-24B-Instruct-2506",
            "temperature": 1,
            "top_k": 100,
            "top_p": 1,
            "repetition_penalty": 1,
            "max_tokens": 1024,
            "prompt": text
        }
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post('http://localhost:5001/v1/completions', 
                                       json=payload) as response:
                    result = await response.json()
                    result_text = result["choices"][0]["text"]
                    
                    if not result_text.startswith("<rephrase>"):
                        result_text = "<rephrase>" + result_text
                    
                    rephrase = None
                    for pattern in rephrase_patterns:
                        match = pattern.search(result_text)
                        if match:
                            rephrase = match.group(1)
                            break
                    
                    if rephrase is None:
                        rephrase = ""
                        
                    return {"text": rephrase, "template": template_name, "success": True}
                    
            except Exception as e:
                print(f"Error processing template {template_name}: {e}")
                return {"text": "", "template": template_name, "success": False, "error": str(e)}

async def process_sample(sample: Dict[str, Any], templates: Dict[str, str], 
                        tokens_per_char: float, semaphore: asyncio.Semaphore) -> List[Dict[str, Any]]:
    """Process a single sample with all templates"""
    # Split into passages
    passages = split_into_passages(sample, tokens_per_char=tokens_per_char)
    
    # Create all tasks for this sample
    tasks = []
    for template_name, template in templates.items():
        for passage in passages:
            prompt = template.format(passage=passage)
            tasks.append(rephrase_chunk(prompt, template_name, semaphore))
    
    # Process tasks with a timeout to prevent hanging
    results = []
    for task in asyncio.as_completed(tasks, timeout=300):  # 5 minute timeout per task
        try:
            result = await task
            results.append(result)
        except asyncio.TimeoutError:
            print("Task timed out")
        except Exception as e:
            print(f"Task failed with error: {e}")
    
    return results

async def main():
    # Estimate tokens per character
    tokens_per_char = estimate_tokens_per_character(dataset, tokenizer)
    print(f"Average Tokens Per Character: {tokens_per_char}")
    
    # Load templates
    templates = {}
    for name in ["easy", "hard", "wiki", "qa"]:
        with open(f"templates/{name}_template.txt") as infile:
            templates[name] = infile.read()
    
    # Initialize result writer
    writer = ResultWriter("rephrasing_results.jsonl")
    
    # Create semaphore to limit concurrent requests
    max_concurrent_requests = 128  # Adjust based on your server capacity
    semaphore = asyncio.Semaphore(max_concurrent_requests)
    
    # Process samples with progress tracking
    total_samples = len(dataset["train"])
    progress = tqdm(total=total_samples, desc="Rephrasing Samples")
    
    # Process samples in smaller batches to avoid memory issues
    batch_size = 10
    for i in range(0, total_samples, batch_size):
        batch = dataset["train"][i:i+batch_size]
        
        # Create tasks for each sample in the batch
        sample_tasks = []
        for sample in batch["text"]:
            sample_tasks.append(process_sample(sample, templates, tokens_per_char, semaphore))
        
        # Process batch samples as they complete
        for task in asyncio.as_completed(sample_tasks):
            try:
                results = await task
                # Write results as they complete
                for result in results:
                    await writer.add_result(result)
                progress.update(1)
            except Exception as e:
                print(f"Error processing sample: {e}")
                progress.update(1)
    
    # Flush any remaining results
    await writer.flush()
    progress.close()

if __name__ == "__main__":
    asyncio.run(main())
