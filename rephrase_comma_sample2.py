import re
import json
import random
import asyncio
import aiohttp
from typing import List, Dict, Any
from tqdm import tqdm
from transformers import AutoTokenizer
from datasets import load_dataset
from collections import defaultdict

# Your existing patterns and tokenizer setup
rephrase_patterns = [re.compile("<rephrase>([\s\S]+)</rephrase>"),
                     re.compile("<rephrase>([\s\S]+)<rephrase>"),
                     re.compile("<rephrase>([\s\S]+)</passage>"),
                     re.compile("<rephrase>([\s\S]+)<passage>"),
                     re.compile("<rephrase>([\s\S]+)</r")]

tokenizer = AutoTokenizer.from_pretrained("common-pile/comma-v0.1-1t")
dataset = load_dataset("jdpressman/comma_v0.1_training_dataset_sample_1B")

def length_order_and_filter_dataset(dataset):
    items = []
    progress = tqdm(total=len(dataset["train"]))
    progress.set_description("Ordering dataset by length")
    for i in range(len(dataset["train"])):
        text = dataset["train"][i]["text"]
        seq_len = len(text)
        if seq_len < 50:
            progress.update(1)
            continue
        # Skip samples which are not prose
        if " " not in text:
            progress.update(1)
            continue
        if ("." not in text
            and "?" not in text
            and "!" not in text):
            progress.update(1)
            continue
        if text.startswith("SEQUENCE LISTING"):
            progress.update(1)
            continue
        if text.startswith('{"type":"FeatureCollection"'):
            progress.update(1)
            continue

        items.append((i, seq_len))
        progress.update(1)
    items.sort(key=lambda x: x[1])
    return items

ordered_indices = [i[0] for i in length_order_and_filter_dataset(dataset)]

def estimate_tokens_per_character(dataset, tokenizer):
    ratios = []
    for i in range(1000):
        choice = random.randrange(len(dataset["train"]))
        char_len = len(dataset["train"][i]["text"])
        token_len = len(tokenizer(dataset["train"][i]["text"])["input_ids"])
        try:
            ratios.append(token_len / char_len)
        except:
            ratios.append(0)
    return sum(ratios) / len(ratios)
        
# This function generated by DeepSeek R1
def split_into_passages(
    text: str,
    max_tokens: int = 350,
    tokens_per_char: float = 0.25,
    min_tokens: int = 50
) -> List[str]:
    """
    Split text into passages following the preprocessing rules in Pieler et al.
    https://arxiv.org/abs/2410.20796
    
    Args:
        text: Input text to split into passages
        max_tokens: Maximum token length for passages (default: 350)
        tokens_per_char: Estimated tokens per character (default: 0.25)
        min_tokens: Minimum token length for passages (default: 50)
    
    Returns:
        List of text passages
    """
    # Helper function to estimate tokens
    def estimate_tokens(chunk: str) -> int:
        return int(len(chunk) * tokens_per_char)
    
    # Helper function to split a long chunk into smaller pieces
    def split_long_chunk(chunk: str, max_tokens: int) -> List[str]:
        # First try to split on sentence boundaries
        sentence_endings = r'(?<=[.!?])\s+'
        sentences = re.split(sentence_endings, chunk)
        
        # If sentences are still too long, split on whitespace
        result = []
        for sentence in sentences:
            if estimate_tokens(sentence) <= max_tokens:
                result.append(sentence)
            else:
                # Fallback: split on whitespace
                words = sentence.split()
                current_chunk = []
                current_length = 0
                
                for word in words:
                    word_tokens = estimate_tokens(word)
                    
                    if current_length + word_tokens > max_tokens and current_chunk:
                        result.append(' '.join(current_chunk))
                        current_chunk = [word]
                        current_length = word_tokens
                    else:
                        current_chunk.append(word)
                        current_length += word_tokens
                
                if current_chunk:
                    result.append(' '.join(current_chunk))
        
        return result
    
    # Step 1: Split on line breaks
    chunks = text.split('\n')
    
    # Step 2: Remove empty passages
    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]
    
    # Step 3: Split chunks exceeding max tokens
    new_chunks = []
    for chunk in chunks:
        if estimate_tokens(chunk) > max_tokens:
            # Use our improved splitting function
            split_chunks = split_long_chunk(chunk, max_tokens)
            new_chunks.extend(split_chunks)
        else:
            new_chunks.append(chunk)
    
    chunks = new_chunks
    
    # Step 4: Merge consecutive passages
    passages = []
    current_passage = []
    current_length = 0
    
    for chunk in chunks:
        chunk_tokens = estimate_tokens(chunk)
        
        # If chunk is too long by itself (shouldn't happen after our improvements)
        if chunk_tokens >= max_tokens:
            if current_passage:
                passages.append(' '.join(current_passage))
                current_passage = []
                current_length = 0
            passages.append(chunk)
            continue
            
        # Check if adding this chunk would exceed max length
        if current_length + chunk_tokens > max_tokens:
            if current_passage:
                passages.append(' '.join(current_passage))
                current_passage = []
                current_length = 0
                
        current_passage.append(chunk)
        current_length += chunk_tokens
    
    # Add the last passage if it meets minimum length
    if current_passage:
        final_passage = ' '.join(current_passage)
        if estimate_tokens(final_passage) >= min_tokens:
            passages.append(final_passage)
    
    return passages

class ResultWriter:
    def __init__(self, output_file: str):
        self.output_file = output_file
        self.buffer = []
        self.buffer_size = 5  # Write to file every 5 results
        
    async def add_result(self, result: Dict[str, Any]):
        self.buffer.append(result)
        if len(self.buffer) >= self.buffer_size:
            await self.flush()
            
    async def flush(self):
        if not self.buffer:
            return
            
        # Use aiofiles for async file writing if needed, or thread pool
        with open(self.output_file, 'a', encoding='utf-8') as f:
            for result in self.buffer:
                f.write(json.dumps(result) + '\n')
        self.buffer.clear()

async def rephrase_chunk(text: str, template_name: str, semaphore: asyncio.Semaphore, index: int, tokens_per_char: float) -> Dict[str, Any]:
    # model_name = "mistralai/Mistral-Small-3.2-24B-Instruct-2506"
    # model_name = "Qwen/Qwen3-4B-Instruct-2507"
    model_name = "Qwen/Qwen3-0.6B"
    async with semaphore:  # Limit concurrent requests
        payload = {
            "model": model_name,
            "temperature": 1,
            "top_k": 100,
            "top_p": 1,
            "repetition_penalty": 1,
            "max_tokens": 1024,
            "prompt": text
        }
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post('http://localhost:5001/v1/completions', 
                                       json=payload) as response:
                    result = await response.json()
                    result_text = result["choices"][0]["text"]
                    
                    if not result_text.startswith("<rephrase>"):
                        result_text = "<rephrase>" + result_text
                    
                    rephrase = None
                    for pattern in rephrase_patterns:
                        match = pattern.search(result_text)
                        if match:
                            rephrase = match.group(1)
                            break
                    
                    if rephrase is None:
                        rephrase = ""

                    progress.update(tokens_per_char * len(rephrase))
                    return {"index": index, "text": rephrase, "template": template_name, "success": True}
                    
            except Exception as e:
                print(f"Error processing template {template_name}: {e}")
                print(result)
                return {"text": "", "template": template_name, "success": False, "error": str(e)}

async def process_sample(sample: Dict[str, Any], templates: Dict[str, str], 
                         tokens_per_char: float, semaphore: asyncio.Semaphore, sample_idx: int) -> List[Dict[str, Any]]:
    """Process a single sample with all templates"""
    print("Yes samples are being processed in parallel")
    # Split into passages
    passages = split_into_passages(sample, tokens_per_char=tokens_per_char)
    
    # Create all tasks for this sample
    tasks = []
    for template_name, template in templates.items():
        index = 0
        for passage in passages:
            prompt = template.format(passage=passage)
            tasks.append(rephrase_chunk(prompt, template_name, semaphore, index, tokens_per_char))
            index += 1
    
    # Process tasks with a timeout to prevent hanging
    chunks = []
    for task in asyncio.as_completed(tasks, timeout=2400):  # 40 minute timeout per task
        try:
            chunk = await task
            chunks.append(chunk)
        except asyncio.TimeoutError:
            print("Task timed out")
        except Exception as e:
            print(f"Task failed with error: {e}")

    results = []
    for template_name in templates:
        template_chunks = [chunk for chunk in chunks if chunk["template"] == template_name]
        template_chunks.sort(key=lambda x: x["index"])
        template_texts = [chunk["text"] for chunk in template_chunks]
        results.append({"index": sample_idx, "text": " ".join(template_texts), "template": template_name}) 
    return results

async def main():
    # Estimate tokens per character
    tokens_per_char = estimate_tokens_per_character(dataset, tokenizer)
    total_tokens = sum([tokens_per_char * len(sample["text"]) for sample in dataset["train"]])
    print(f"Average Tokens Per Character: {tokens_per_char}")
    print(f"Estimated Total Tokens: {total_tokens}")
    
    # Load templates
    templates = {}
    for name in ["easy", "hard", "wiki", "qa"]:
        with open(f"templates/{name}_template.txt") as infile:
            templates[name] = infile.read()
    
    # Initialize result writer
    writer = ResultWriter("rephrasing_results.jsonl")
    
    # Create semaphore to limit concurrent requests
    max_concurrent_requests = 512  # Adjust based on your server capacity
    semaphore = asyncio.Semaphore(max_concurrent_requests)
    
    # Process samples with progress tracking
    total_samples = len(dataset["train"])
    global progress
    progress = tqdm(total=total_tokens * 4, desc="Rephrasing Tokens")
    
    # Process samples in smaller batches to avoid memory issues
    batch_size = 32
    for i in range(0, total_samples, batch_size):
        batch = []
        for _ in range(batch_size):
            index = ordered_indices.pop()
            batch.append((dataset["train"][index]["text"], index))
        
        # Create tasks for each sample in the batch
        sample_tasks = []
        for sample, index in batch:
            sample_tasks.append(process_sample(sample, templates, tokens_per_char, semaphore, index))
        
        # Process batch samples as they complete
        for task in asyncio.as_completed(sample_tasks):
            try:
                results = await task
                # Write results as they complete
                for result in results:
                    await writer.add_result(result)
            except Exception as e:
                print(f"Error processing sample: {e}")
    
    # Flush any remaining results
    await writer.flush()
    progress.close()

if __name__ == "__main__":
    import pdb
    pdb.set_trace()
    asyncio.run(main())
